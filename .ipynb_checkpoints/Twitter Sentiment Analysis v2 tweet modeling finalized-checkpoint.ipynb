{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "### Search through Twitter API ###\n",
    "\n",
    "# Import Packages\n",
    "import tweepy\n",
    "import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tweetpreprocess as tp\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lmtzr = nltk.WordNetLemmatizer()\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Keys\n",
    "my_bearer_token = config.BEARER_TOKEN\n",
    "\n",
    "# Get token\n",
    "client = tweepy.Client(bearer_token=my_bearer_token)\n",
    "\n",
    "# API query and response\n",
    "query = '(donald trump) -is:retweet lang:en'\n",
    "# response = client.search_recent_tweets(query=query, max_results=101)\n",
    "\n",
    "# Dataset placeholder\n",
    "data = []\n",
    "\n",
    "# Fill data\n",
    "for tweet in tweepy.Paginator(client.search_recent_tweets, query=query, max_results=100).flatten(limit=3000):\n",
    "    data.append(tweet.text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up dataset ###\n",
    "# Create test dataset\n",
    "columns = ['text']\n",
    "test_data = pd.DataFrame(data, columns=columns)\n",
    "test_data['text'] = test_data['text'].str.decode(\"utf-8\")\n",
    "test_data.to_excel(excel_writer = \"test_tweets.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre-processing ###\n",
    "from textblob import TextBlob\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Check the polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def getAnalysis(score):\n",
    "    if score < 0:\n",
    "        return '-1'\n",
    "    elif score == 0:\n",
    "        return '0'\n",
    "    else:\n",
    "        return '1'\n",
    "\n",
    "# Excel to Dataframe (for saving purposes)\n",
    "full_test_df = pd.read_excel('test_tweets.xlsx')\n",
    "test_tweet_df = full_test_df[[\"text\"]]\n",
    "test_tweet_df[\"text\"] = test_tweet_df[\"text\"].astype(str)\n",
    "\n",
    "# Tweet pre-processing \n",
    "def PreProcessTweets(tweet_df):\n",
    "    # labeling with Polarity\n",
    "    tweet_df[\"Label\"] = tweet_df['text'].apply(getPolarity)\n",
    "    # Labeling the Analysis\n",
    "    tweet_df[\"Analysis\"] = tweet_df['Label'].apply(getAnalysis)\n",
    "    # Lowercasing\n",
    "    tweet_df[\"text\"] = tweet_df[\"text\"].str.lower()\n",
    "    # Remove URLs\n",
    "    tweet_df[\"text\"] = tweet_df[\"text\"].apply(lambda text: tp.remove_urls(text))\n",
    "    # Remove Hashtags\n",
    "    tweet_df[\"text\"] = tweet_df[\"text\"].apply(lambda text: tp.remove_hashtags(text))\n",
    "    # Remove usernames\n",
    "    tweet_df[\"text\"] = tweet_df[\"text\"].apply(lambda text: tp.remove_users(text))\n",
    "    # Convert emojis to words\n",
    "    tweet_df[\"text\"] = tweet_df[\"text\"].apply(lambda text: tp.emojiToWord(text))\n",
    "    # Remove extra whitespaces\n",
    "    tweet_df[\"text\"] = tweet_df[\"text\"].apply(lambda text: tp.remove_extraws(text))\n",
    "    # Spell checker\n",
    "    tweet_df[\"text\"] = tweet_df[\"text\"].apply(lambda text: tp.correct_spellings(text))\n",
    "    # Tokenizer\n",
    "    tweet_df[\"text\"] = tweet_df[\"text\"].apply(lambda text: nltk.word_tokenize(text, language='english'))\n",
    "    # Lemmatizer\n",
    "    tweet_df[\"text\"] = tweet_df[\"text\"].apply(lambda lst:[lmtzr.lemmatize(word) for word in lst])\n",
    "    # Remove stopwords\n",
    "    tweet_df[\"text\"] = tweet_df[\"text\"].apply(lambda text: tp.remove_sw(text))\n",
    "    return tweet_df\n",
    "\n",
    "preproc_test = PreProcessTweets(test_tweet_df)\n",
    "preproc_test.to_excel(excel_writer = \"preprocessed lemma.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modeling ###\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "model_train_df= pd.read_excel(\"preprocessed lemma.xlsx\")\n",
    "\n",
    "# Flattening\n",
    "flattened = []\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='unicode', stop_words=stopset)\n",
    "y = model_train_df.Analysis\n",
    "X = vectorizer.fit_transform(model_train_df.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000,)\n",
      "(3000, 5613)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modeling pt.2 ###\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "clf = naive_bayes.MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.708\n"
     ]
    }
   ],
   "source": [
    "# ROC \n",
    "y_pred_class = clf.predict(X_test)\n",
    " \n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_array=np.array([\"\"])\n",
    "test_vector = vectorizer.transform(test_array)\n",
    "print(clf.predict(test_vector))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e568e4913ed3a5418050f05b8ec0348cfe31469c376a761d24cc10ae1d3b12bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
